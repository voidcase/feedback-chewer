# proposals: 2061

# beamlines: 19
# departments: 199
# user affiliations: 174

# unique words in total: 3917 (after stopwords removal:3664)
(# words after min_df of 0.02: 47) 
# unique words in experiment comments: 2363
# unique words in infrastructure comments: 960
# unique words in overall comments: 1654
# unique words in remarks: 436

# experiment comments: 662
# infrastructure comments: 275
# overall comments: 397
# remarks: 80

Accuracies with default configuration (values are rounded off average of 5 runs cv):

SVM (linear kernel): 0.38
SVM (default): 0.4
linear regression with rounding off: 0.35
linear regression without rounding off: (R^2) = +- 0.0
logistic regression: 0.42

Accuracies with tf instead of tfidf:

SVM (linear kernel): 0.4
SVM (default): 0.4
linear regression with rounding off: 0.2
linear regression without rounding off: (R^2) = +- number * 10^24 or in that category
logistic regression: 0.38

Overall grades:

5: 775 times
4: 825 times
3: 252 times
2: 81 times
1: 26 times
0: 103 times -> some people did not answer anything or forgot the overall grade

number of words in word embedding corpus: 1376399


STATISTICS WITH BINARIZING, +weighted wordembedding
f1 scores:
random forest:  0.9
Logistic regression:  0.9
Logisic regression without wordembedding:  0.9
SVM without wordembedding:  0.9
SVM with wordembedding:  0.9
Extra trees: 0.9

precision:
random forest:  0.82
Logistic regression:  0.84
Logistic regression without wordembedding:  0.82
SVM without wordembedding:  0.82
SVM with wordembedding   :  0.82
Extra trees: 0.83

recall:
random forest:  0.98
Logistic regression:  0.93
Logistic regression without wordembedding:  0.98
SVM without wordembedding:  1
SVM with wordembedding:  1
Extra trees: 0.96

Lowest Target F1 score (average of 3-fold CV)
-------------------------------------------------------------
OWN DICT + WEIGHING
svm
	 0.62
logistic regression
	 0.62
GaussianNB
	 0.63
extra trees
	 0.66
random forest
	 0.67

OWN DICT NO WEIGHING
svm
	 0.6
logistic regression
	 0.6
GaussianNB
	 0.6
extra trees
	 0.64
random forest
	 0.66

DICT WEIGHING
svm
	 0.62
logistic regression
	 0.66
GaussianNB
	 0.63
extra trees
	 0.63
random forest
	 0.63
adaboost
     0.62

DICT NO WEIGHING
svm
	 0.62
logistic regression
	 0.62
GaussianNB
	 0.62
extra trees
	 0.62
random forest
	 0.63


NO WORDEMBEDDINGS (f1)
svm
	 0.52
logistic regression without wordembedding
	 0.64
GaussianNB without wordembedding
	 0.6
extra trees without wordembedding
	 0.63
random forest without wordembedding
	 0.63
adaboost
     0.62


adaboost
	 [ 0.63369397  0.67503137  0.67481663]
logistic regression
	 [ 0.59432387  0.66839378  0.69961977]
GaussianNB
	 [ 0.55062167  0.65863454  0.67282322]
extra trees
	 [ 0.591133    0.67783505  0.70332481]
random forest
	 [ 0.61341853  0.67605634  0.70113493]
svm
	 [ 0.55062167  0.65863454  0.67282322]


SUMMARY 23/11
No difference in accuracy/f1 score when you add wordembeddings, except for svm, logistic regression
however, the precision gets a bit better and the recall a bit worse
When we train our own wordembeddings, weighing the vectors with the IDF seems to help a bit,
but with the existing wordembeddings, it does not seem to add anything.
our own wordembeddings + weighing seems to work equally good as the existing wordembeddings (either with or without weighing)

Binarizing
When we binarize the overall scores by >3 and <3 and dividing 3 randomly, there are almost no negative ones so the predictions get really good,
but this does not say a lot
When we binarize the lowest scores, there are more negative ones, but the accuracy is only around 60%
When we binarize the average scores, there are almost no negative ones again...

Autocorrect
autocorrect only seems to help for certain classifiers, and only helps around 1%, but classifiers do not get worse.

We deleted rows with grade 0, and we do not use the other features.

logistic regression with w2v
	 [ 0.71942446  0.71582734  0.71223022]
GaussianNB with w2v
	 [ 0.6618705   0.61870504  0.66906475]
extra trees with w2v
	 [ 0.69784173  0.68705036  0.68705036]
random forest with w2v
	 [ 0.70863309  0.73021583  0.72302158]
svm with w2v
	 [ 0.6618705  0.6618705  0.6618705]